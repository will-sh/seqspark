#!/usr/bin/env python3
"""
SeqSpark Workaround - å®Œå…¨ç»•è¿‡HDFSå†™å…¥é—®é¢˜çš„ç‰ˆæœ¬
"""

import argparse
import sys
import os
import time
from pyspark.sql import SparkSession

def parse_fastq_partition(lines):
    lines_list = list(lines)
    records = []
    i = 0
    while i + 3 < len(lines_list):
        if lines_list[i].startswith('@'):
            header = lines_list[i][1:].strip()
            sequence = lines_list[i + 1].strip()
            quality = lines_list[i + 3].strip()
            records.append((header, sequence, quality))
            i += 4
        else:
            i += 1
    return records

def format_fastq_record(record):
    header, sequence, quality = record
    return f"@{header}\n{sequence}\n+\n{quality}"

def main():
    parser = argparse.ArgumentParser(description="SeqSpark Workaround - ç»•è¿‡HDFSé—®é¢˜")
    parser.add_argument('-i', '--input', required=True, help='è¾“å…¥FASTQæ–‡ä»¶')
    parser.add_argument('-o', '--output', default='-', help='è¾“å‡ºæ–‡ä»¶')
    parser.add_argument('-p', '--proportion', type=float, required=True, help='é‡‡æ ·æ¯”ä¾‹')
    parser.add_argument('-s', '--seed', type=int, default=11, help='éšæœºç§å­')
    parser.add_argument('--master', default='local[*]', help='Spark master')
    parser.add_argument('--memory', default='4g', help='å†…å­˜é…ç½®')
    
    args = parser.parse_args()
    
    spark = SparkSession.builder \
        .appName("SeqSpark-Workaround") \
        .master(args.master) \
        .config("spark.driver.memory", args.memory) \
        .config("spark.executor.memory", args.memory) \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    
    try:
        print(f"ðŸš€ SeqSpark Workaround - å¤„ç†æ–‡ä»¶: {args.input}")
        print(f"ðŸ“Š é‡‡æ ·æ¯”ä¾‹: {args.proportion}")
        
        # å¤„ç†æ•°æ®
        lines_rdd = spark.sparkContext.textFile(args.input)
        records_rdd = lines_rdd.mapPartitions(parse_fastq_partition)
        sampled_rdd = records_rdd.sample(False, args.proportion, args.seed)
        output_rdd = sampled_rdd.map(format_fastq_record)
        
        # ä½¿ç”¨collect()èŽ·å–ç»“æžœ
        print("ðŸ“¦ ä½¿ç”¨collect()æ”¶é›†ç»“æžœ...")
        results = output_rdd.collect()
        print(f"âœ… æˆåŠŸæ”¶é›† {len(results)} æ¡é‡‡æ ·è®°å½•")
        
        # è¾“å‡ºå¤„ç†
        if args.output == '-':
            for result in results:
                print(result)
        else:
            # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶å
            if args.output.startswith(('hdfs://', '/user/')):
                local_file = f"/tmp/seqspark_workaround_{int(time.time())}.fastq"
                print(f"âš ï¸  HDFSè·¯å¾„æ£€æµ‹åˆ°ï¼Œæ”¹ä¸ºæœ¬åœ°æ–‡ä»¶: {local_file}")
            else:
                local_file = args.output
            
            # å†™å…¥æœ¬åœ°æ–‡ä»¶
            with open(local_file, 'w') as f:
                for result in results:
                    f.write(result + '\n')
            
            print(f"âœ… æˆåŠŸå†™å…¥: {local_file}")
            
            # å¦‚æžœæ˜¯HDFSè·¯å¾„ï¼Œæä¾›å¤åˆ¶å‘½ä»¤
            if args.output.startswith(('hdfs://', '/user/')):
                print(f"ðŸ“‹ æ‰‹åŠ¨å¤åˆ¶åˆ°HDFSå‘½ä»¤:")
                print(f"   hdfs dfs -put {local_file} {args.output}")
                print(f"   hdfs dfs -rm {local_file}  # æ¸…ç†æœ¬åœ°æ–‡ä»¶")
        
        print(f"ðŸŽ‰ ä»»åŠ¡å®Œæˆï¼")
        
    except Exception as e:
        print(f"ðŸ’¥ ä»»åŠ¡å¤±è´¥: {str(e)}", file=sys.stderr)
        sys.exit(1)
    finally:
        spark.stop()

if __name__ == "__main__":
    main() 